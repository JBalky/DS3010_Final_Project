# -*- coding: utf-8 -*-
""" 
Created on Mon Feb 20 10:29:18 2023

@author: JACK
"""

import pandas as pd # data processing
import sqlite3
import os
import statsmodels.api as sm
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
#from xgboost import plot_importance
from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline





#Dataset is huge. 
#pd.set_option('display.max_rows', 1500)
#pd.set_option('display.max_columns', 1000)
#pd.set_option('display.width', 1000)
        
# Create a SQL connection to our SQLite database
con = sqlite3.connect("C:/Users/JACK/Downloads/SWITRS_Data/switrs.sqlite")

cur = con.cursor()

#WHERE strftime('%Y', collision_date) == '2018' OR strftime('%Y', collision_date) == '2019' OR strftime('%Y', collision_date) == '2020'
#This is where we make a query
collisions_query = pd.read_sql_query("SELECT case_id, county_location, location_type, collision_severity, collision_date FROM collisions", con)
parties_query = pd.read_sql_query("SELECT case_id, party_sex, party_age, direction_of_travel, vehicle_make, party_race FROM parties WHERE at_fault == '1' ", con)

#Now that the dataframes are made. We need to clean the data up. First, we remove the missing data. 

collisionDF = collisions_query.dropna()
partiesDF = parties_query.dropna()

#Then we set the index to be case_id. We will only use case_id found in both dataset
collisionDF = collisionDF.set_index('case_id')
partiesDF = partiesDF.set_index('case_id')
common_indexes = collisionDF.index.intersection(partiesDF.index)
collisionDF = collisionDF.loc[common_indexes]
partiesDF = partiesDF.loc[common_indexes]


#collisionDF.head(20)
#partiesDF.head(20)
#len(collisionDF)
#len(partiesDF)
#Exploratory analysis

#partiesDF.party_sex.hist()
#partiesDF.vehicle_make.hist()
#partiesDF.party_race.hist()

# We have severity of collision: Fatal, pain, property damage, severe injury, other injury. We will
# condense this into fatal, propterty damage, and injury.

def fiveToThreeResponse(x):
    if x == 'pain' or x == 'other injury' or x == 'severe injury':
        return 'injury'
    return x


#example = {'case_id': [1,2,3,4,5,6,7,8,9,10], 'collision_severity': ['fatal', 'property damage only', 'pain', 'severe injury', 'other injury', 'pain', 'severe injury', 'other injury', 'fatal', 'property damage only']}
#exampledf = pd.DataFrame(example)
#exampledf

collisionDF['collision_severity'] = collisionDF['collision_severity'].apply(fiveToThreeResponse)

#collisionDF.isnull().sum()
#partiesDF.isnull().sum()

print(collisionDF.index.equals(partiesDF.index))  # True


#Combine both datasets
df_concatenated = pd.concat([collisionDF, partiesDF], axis=1)

#GOOD

#Cleans data set. Only includes Male and Female
df_concatenated = df_concatenated.drop(df_concatenated.loc[df_concatenated['party_sex']=='X'].index)
#print(df_concatenated['party_sex'].unique())

#df_concatenated_BACKUP = df_concatenated


#Turn age into bins
bins = [18, 25, 35, 45, float('inf')]
labels = ['18-25', '26-35', '36-45', '56+']
df_concatenated['party_age'] = pd.cut(df_concatenated['party_age'], bins=bins, labels=labels)
df_concatenated = df_concatenated.dropna(subset=['party_age'])


#Only use top 5 vehicle makes
top_5_vehicle_makes = df_concatenated['vehicle_make'].value_counts().nlargest(5).index.tolist()
df_concatenated = df_concatenated[df_concatenated['vehicle_make'].isin(top_5_vehicle_makes)]
STORE = df_concatenated
#print(df_concatenated['party_race'].unique())
#print(df_concatenated.isnull().sum())


#Encode the categorical variables
df_encoded = pd.get_dummies(df_concatenated, columns=['party_sex', 'vehicle_make', 'party_race'])

label_encoder = LabelEncoder()
df_encoded['party_age'] = label_encoder.fit_transform(df_encoded['party_age'])
#df_encoded.columns

X = df_encoded.drop(['county_location', 'location_type', 'collision_severity', 'collision_date', 'party_age', 'direction_of_travel'], axis=1)
y = df_encoded['collision_severity']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

over = SMOTE(sampling_strategy='auto', k_neighbors=5)
under = RandomUnderSampler(sampling_strategy='auto')
steps = [('over', over), ('under', under)]
pipeline = Pipeline(steps=steps)
X_train_res, y_train_res = pipeline.fit_resample(X_train, y_train)

# Train models
models = [
    ('Decision Tree', DecisionTreeClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('Logistic Regression', LogisticRegression()),
    ('SVM', SVC())
]

for name, model in models:
    print(f'Training {name}...')
    if name == 'SVM':
        # SVM requires scaling of data
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_res)
        X_test_scaled = scaler.transform(X_test)
        model.fit(X_train_scaled, y_train_res)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train_res, y_train_res)
        y_pred = model.predict(X_test)
    print(f'{name} results:')
    #print('Accuracy:', model.score(X_test, y_test))
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))


#RANDOM FOREST
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)
print("Accuracy: ", accuracy)
print("Confusion matrix: \n", confusion)





#We have to check how many estimators would give best result

# param_grid = {
#     'n_estimators': [10, 50, 100, 150, 200, 250, 300]
# }

# grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)
# grid_search.fit(X_train, y_train)
# print("Best Parameters: ", grid_search.best_params_)
# print("Best Score: ", grid_search.best_score_)





# #DECISION TREE
# dt_model = DecisionTreeClassifier(random_state=42)
# dt_model.fit(X_train, y_train)
# y_pred = dt_model.predict(X_test)
# accuracy = accuracy_score(y_test, y_pred)
# confusion = confusion_matrix(y_test, y_pred)
# print("Accuracy: ", accuracy)
# print("Confusion matrix: \n", confusion)

# # Train the model using logistic regression
# model = LogisticRegression()
# model.fit(X_train, y_train)

# # Predict the collision severity on the testing data
# y_pred = model.predict(X_test)

# # Evaluate the model's performance
# print(classification_report(y_test, y_pred))
# print('Accuracy:', model.score(X_test, y_test))
# print(confusion_matrix(y_test, y_pred))

# plot_importance(model)
# plt.show()
# #I have a data frame which has categorical variables sex (male/female), vehicle make (10 different cars), race (white, black, Hispanic, Asian) and ordinal variable age(18-25, 26-35, 36-45, 56+) and response variable collision severity (fatal, property damage, injury). Devlop a machine learning model using the variables to predict collision severity












